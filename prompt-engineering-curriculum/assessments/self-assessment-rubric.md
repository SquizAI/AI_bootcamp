<div align="center">

[![Typing SVG](https://readme-typing-svg.herokuapp.com?font=Fira+Code&size=28&duration=3000&pause=1000&color=6C63FF&center=true&vCenter=true&width=600&lines=Self-Assessment+%26+Reflection;Master+Your+Prompt+Engineering;Grow+Through+Deliberate+Practice)](https://github.com/SquizAI)

<br>

[![Self-Directed Learning](https://img.shields.io/badge/Self--Directed-Learning-FF6B6B?style=for-the-badge&logo=books&logoColor=white)](https://github.com/SquizAI)
[![Reflection Tool](https://img.shields.io/badge/Reflection-Tool-4ECDC4?style=for-the-badge&logo=lightbulb&logoColor=white)](https://github.com/SquizAI)
[![Growth Mindset](https://img.shields.io/badge/Growth-Mindset-FFD93D?style=for-the-badge&logo=trending-up&logoColor=white)](https://github.com/SquizAI)

<br>

![Self-Assessment Banner](https://user-images.githubusercontent.com/33300832/210897152-8fa0f3a7-bc2d-4c62-a1a7-1b6b4f2e4f8e.gif)

</div>

---

## Overview

This self-assessment rubric is your personal guide to evaluating prompt quality and tracking your growth as a prompt engineer. Use it after crafting each prompt to identify strengths, pinpoint improvement areas, and celebrate progress.

> **Why Self-Assessment Matters:** Reflection transforms experience into learning. By evaluating your own work, you develop the metacognitive skills that accelerate mastery.

---

## How to Use This Rubric

<div align="left">

1. **Write your prompt** - Create a prompt for your current learning task
2. **Evaluate against criteria** - Score your prompt on the five key dimensions below
3. **Record your ratings** - Use the scale: Beginner (B) → Developing (D) → Proficient (P) → Advanced (A)
4. **Reflect deeply** - Answer the reflection questions to understand your choices
5. **Track patterns** - Monitor your growth across multiple prompts to spot learning trajectories
6. **Plan improvements** - Identify which criterion would have the biggest impact next time

</div>

---

## Five Criteria for Prompt Evaluation

### Criterion 1: Role Assignment
**Who should the AI be?**

<table>
<tr>
<td width="25%">

**Advanced** ✓✓✓✓✓

</td>
<td>

Specific, detailed role with clear expertise and approach style defined.

- You specify exactly who the AI should be (teacher, tutor, editor, coach, expert, mentor)
- You include expertise level or approach style
- **Example:** *"Act as a patient biology tutor who uses real-world examples and breaks down concepts for visual learners"*
- **Result:** AI adopts the precise persona and responds with appropriate tone and depth

</td>
</tr>

<tr>
<td>

**Proficient** ✓✓✓✓

</td>
<td>

Clear role definition with relevant context.

- You tell the AI who to be with appropriate subject context
- Role is specific enough to guide response style
- **Example:** *"Act as a math tutor"*
- **Result:** AI responds in an appropriate expert voice

</td>
</tr>

<tr>
<td>

**Developing** ✓✓✓

</td>
<td>

Role is implied but not explicitly stated.

- You hint at a role ("help me understand," "teach me") without stating it directly
- Role is vague or generic
- **Example:** *"Explain how photosynthesis works"*
- **Result:** AI responds but might not match your intended tone or depth

</td>
</tr>

<tr>
<td>

**Beginner** ✓✓

</td>
<td>

No role assigned or defined.

- Your prompt doesn't specify who the AI should be
- Direct question without persona context
- **Example:** *"What is photosynthesis?"*
- **Result:** Generic response at potentially wrong level or tone

</td>
</tr>
</table>

---

### Criterion 2: Context Provision
**What does the AI need to know?**

<table>
<tr>
<td width="25%">

**Advanced** ✓✓✓✓✓

</td>
<td>

Rich, specific contextual information about your situation.

- You share your current knowledge level AND specific knowledge gaps
- You explain what you're struggling with specifically
- You mention learning preferences or assignment requirements
- **Example:** *"I'm a 9th grader. I understand plants need sunlight and water, but I don't understand what's happening in the cells. I learn best with analogies rather than technical jargon, and this is for my biology midterm."*
- **Result:** AI tailors response precisely to your learning situation

</td>
</tr>

<tr>
<td>

**Proficient** ✓✓✓✓

</td>
<td>

Relevant context provided about your learning situation.

- You mention your level and current knowledge state
- You explain why you're learning this
- **Example:** *"I'm a 7th grader and I don't understand photosynthesis for my science class"*
- **Result:** AI responds at appropriate level

</td>
</tr>

<tr>
<td>

**Developing** ✓✓✓

</td>
<td>

Incomplete or vague contextual information.

- You provide limited contextual information
- Context is mentioned but not specific
- **Example:** *"I'm learning about photosynthesis but confused"*
- **Result:** AI makes assumptions that might miss your actual needs

</td>
</tr>

<tr>
<td>

**Beginner** ✓✓

</td>
<td>

Minimal or no context provided.

- Missing grade level, prior knowledge, or specific challenge
- Only the topic provided
- **Example:** *"Tell me about photosynthesis"*
- **Result:** Generic response that may be too simple, too complex, or mismatched to your needs

</td>
</tr>
</table>

---

### Criterion 3: Task Definition
**What exactly do you want?**

<table>
<tr>
<td width="25%">

**Advanced** ✓✓✓✓✓

</td>
<td>

Precise outcome specified with clear intent.

- You're specific about the type of help needed (explain concept, check work, generate practice, give feedback, compare approaches)
- You state the purpose clearly
- You distinguish between different types of understanding
- **Example:** *"Help me understand WHY the process works, not just HOW to do it. I want to grasp the underlying mechanism."*
- **Result:** AI focuses exactly on your specific learning need

</td>
</tr>

<tr>
<td>

**Proficient** ✓✓✓✓

</td>
<td>

Clear main request without ambiguity.

- Your request is understandable and specific
- Purpose is clear
- **Example:** *"Explain photosynthesis to me"*
- **Result:** AI knows what you're asking for

</td>
</tr>

<tr>
<td>

**Developing** ✓✓✓

</td>
<td>

Somewhat unclear what you're really asking for.

- Your task is implied but not explicitly stated
- Multiple interpretations possible
- **Example:** *"Tell me about photosynthesis"*
- **Result:** AI might misunderstand the type of help you actually need

</td>
</tr>

<tr>
<td>

**Beginner** ✓✓

</td>
<td>

Vague or unclear request.

- Unclear what you actually want or why
- Could mean many different things
- **Example:** *"Photosynthesis?"*
- **Result:** AI guesses, often incorrectly

</td>
</tr>
</table>

---

### Criterion 4: Format Specification
**How should the response be structured?**

<table>
<tr>
<td width="25%">

**Advanced** ✓✓✓✓✓

</td>
<td>

Detailed format preferences matched to your learning style.

- You explain exactly how you want the response organized
- You request specific structures (step-by-step, bullet points, test questions, analogies, visual descriptions)
- You're intentional about the format that helps you learn best
- **Example:** *"Explain this with a daily life analogy first, then show the actual scientific process. Use numbered steps and bold the key terms."*
- **Result:** Response matches how you learn best

</td>
</tr>

<tr>
<td>

**Proficient** ✓✓✓✓

</td>
<td>

Format preference clearly stated.

- You request a specific structure
- Format choice is logical for your need
- **Example:** *"Explain it step-by-step" or "Use bullet points"*
- **Result:** Response is organized helpfully

</td>
</tr>

<tr>
<td>

**Developing** ✓✓✓

</td>
<td>

Format preference is vague or implied.

- You suggest format vaguely ("explain simply," "be clear") without specifying structure
- Format preference is unclear
- **Example:** *"Keep it short"*
- **Result:** AI tries but might not match your actual preference

</td>
</tr>

<tr>
<td>

**Beginner** ✓✓

</td>
<td>

No format specified or considered.

- Nothing about response structure or organization
- Example: Just the question
- **Result:** AI chooses format that may not be optimal for your learning

</td>
</tr>
</table>

---

### Criterion 5: Constraints
**What should the AI NOT do?**

<table>
<tr>
<td width="25%">

**Advanced** ✓✓✓✓✓

</td>
<td>

Clear, strategic constraints preventing unhelpful responses.

- You use multiple relevant constraints to keep AI within helpful boundaries
- Constraints protect your learning process
- Constraints are specific, not vague
- **Example:** *"Don't give me the answer directly—guide my thinking with questions. Explain at 9th grade level. Focus only on aerobic respiration. Avoid technical jargon."*
- **Result:** AI stays within boundaries that support your learning

</td>
</tr>

<tr>
<td>

**Proficient** ✓✓✓✓

</td>
<td>

Relevant constraints included and clear.

- You specify what the AI should avoid
- Constraints support your learning goal
- **Example:** *"Don't solve this for me—help me understand how to solve it"*
- **Result:** AI respects your learning goal

</td>
</tr>

<tr>
<td>

**Developing** ✓✓✓

</td>
<td>

Constraint is vague or partially implied.

- Constraint is mentioned but not specific
- Intent is unclear
- **Example:** *"Keep it simple"*
- **Result:** AI tries but might misinterpret your intent

</td>
</tr>

<tr>
<td>

**Beginner** ✓✓

</td>
<td>

No constraints included or mentioned.

- Nothing about what the AI shouldn't do
- Example: No negative prompting at all
- **Result:** AI might give direct answers, be too complex, use wrong format, go off-topic, etc.

</td>
</tr>
</table>

---

## Quick Quality Check

<div align="center">

### After evaluating each criterion, ask yourself:

| Element | Checklist |
|---------|-----------|
| **Role** | Does my prompt have a clear, specific role for the AI? |
| **Context** | Have I provided context about who I am and what I know? |
| **Task** | Is my request/task specific and unambiguous? |
| **Format** | Have I specified how I want the response structured? |
| **Constraints** | Are there important boundaries about what the AI shouldn't do? |

</div>

### Assessment Guide

- **5 checked** - Your prompt is solid and should get excellent results. You're thinking like a prompt engineer.
- **3-4 checked** - Good foundation. Identify which missing element would help most.
- **1-2 checked** - That's your learning opportunity. Which criterion would have the biggest impact if improved?
- **0 checked** - Perfect timing to focus on fundamentals. Start with Role or Task.

---

## Reflection Questions

<div align="left">

After writing and testing a prompt, move through these reflection questions to deepen your understanding.

### About the Process

1. **What was easy about writing this prompt?** (Which criterion felt natural—Role assignment? Context? Constraints?)
2. **What was challenging?** (Which aspect required the most thought?)
3. **Which part took the most thought?** (What indicates importance to you?)

### About the Response

4. **Did the AI understand what you needed?** How do you know?
5. **What parts of the response were most helpful?** Why did those parts work?
6. **What could have been better about the response?** How could the prompt have guided it better?

### About Improvement

7. **If you could revise the prompt, what would you change?** What would you do differently?
8. **Which criterion would most improve the response if you enhanced it?** (Role? Context? Task? Format? Constraints?)
9. **What will you do differently next time?** How will you apply this lesson?

### About Transfer

10. **How could you use a similar prompt with a different subject?** What parts transfer?
11. **What did you learn about prompt engineering from this experience?** What's the big idea?

</div>

---

## Progress Tracking

Keep a record of your prompts and scores to observe growth patterns over time. This data reveals how you're developing as a prompt engineer.

### Typical Growth Patterns

**Early Prompts** typically show:
- Weaker Context (not providing enough background about your situation)
- Weaker Constraints (not thinking proactively about prevention)
- Stronger Role and Task (more intuitive to most people)

**Later Prompts** typically show:
- All criteria improving steadily
- More intentional, deliberate choices
- Faster iteration and refinement
- Better anticipation of AI needs before problems occur
- Combination of techniques (constraints + format, context + task)

### Track Your Growth with This Table

| Prompt # | Topic | Role | Context | Task | Format | Constraints | Total | Notes |
|:--------:|:-----:|:----:|:-------:|:----:|:------:|:-----------:|:-----:|:------|
| 1 | Math | P | D | P | D | B | 3/5 | Role clear but no constraints |
| 2 | Science | P | P | P | P | D | 4/5 | Getting better |
| 3 | Writing | A | P | A | P | P | 5/5 | Much improved |
| 4 | History | A | A | A | A | P | 5/5 | Consistent excellence |

**Key:** A = Advanced | P = Proficient | D = Developing | B = Beginner

---

## Common Growth Patterns & Next Steps

### "I'm getting better at Context"
- **What this shows:** You're thinking deeply about what information the AI needs
- **The impact:** You're personalizing prompts to your specific situation and background
- **Next step:** Combine stronger context with stronger constraints for maximum effect

### "My Format is improving"
- **What this shows:** You're thinking intentionally about how you learn best
- **The impact:** You understand that structure and presentation directly affect learning
- **Next step:** Try different formats for different tasks—find your preferences

### "Constraints are still weak"
- **What this shows:** This is normal—constraints are the hardest part to develop
- **The impact:** You're learning to think proactively about prevention
- **Next step:** Add just one constraint to your current assignment as practice

### "Everything is improving"
- **What this shows:** You're internalizing prompt engineering principles
- **The impact:** You're developing the mindset of an effective AI communicator
- **Next step:** Experiment with advanced techniques (multi-step prompts, few-shot examples, chain-of-thought)

### "I'm stuck on one criterion"
- **What this shows:** You've identified a specific learning edge
- **The impact:** Focused improvement beats random practice
- **Next step:** Create 3 practice prompts focusing only on that criterion

---

## Subject-Specific Applications

Different subjects benefit from different emphasis:

| Subject | Key Strength to Develop | Why | Example Focus |
|---------|------------------------|-----|---|
| **Math** | Strong Constraints | Prevents AI from solving instead of teaching | "Don't give the answer—guide me step by step" |
| **Science** | Rich Context | Connects to what you already know | "I understand X, but not Y, and here's why I think..." |
| **Writing** | Format + Role | Feedback style matters significantly | "Give feedback as an editor would, not a teacher" |
| **History** | Multi-step Tasks | Complex analysis requires scaffolding | "First causes, then effects, then connections" |
| **Languages** | Constraints + Format | Prevents translation, encourages production | "Respond only in [language]. Don't translate." |
| **All Subjects** | Intentional Variety | Different tasks need different emphasis | Rotate which criteria you focus on developing |

---

## Remember: This Is Your Learning Journey

<div align="center">

> This rubric is **for your learning, not for judgment**.

</div>

- Scoring "Developing" is good—it shows you know what to improve
- You'll get better with consistent practice and reflection
- Even experienced prompt engineers revise and refine their work
- The goal is progress, not perfection
- Every prompt teaches you something valuable

**Key Insight:** Your best prompts will be for topics you care about, assignments that truly challenge you, and subjects where you have relevant context. This is intentional—you're most thoughtful when you have genuine motivation.

---

## Your Next Steps

1. **Choose a current assignment** - Pick something you're working on right now
2. **Write your best prompt** - Use what you know about all five criteria
3. **Score yourself** - Be honest about where you stand
4. **Answer 3 reflection questions** - Pick the ones most relevant to you
5. **Try one improvement** - Focus on one criterion that would help most
6. **Write and test** - See what happens with the improved prompt
7. **Reflect again** - What changed? What did you learn?

Keep practicing. You're learning a skill that will help you throughout your education and beyond. Prompt engineering is about clear thinking, and clear thinking is about understanding what you want and why. That's a skill worth developing.

**Your growth is the evidence of your learning.**

---

<div align="center">

**Made with focus on your learning journey** | Last updated: 2025

</div>